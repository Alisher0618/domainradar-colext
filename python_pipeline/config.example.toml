[connection]
brokers = ["aiokafka://localhost:9092"]
use_ssl = false

[connection.ssl]
ca_file = "/path/to/ca.pem"
server_verification_required = true
client_cert_file = "/path/to/client_certificate.pem"
client_key_file = "/path/to/client_key.pem"
client_key_password = "password for the private key"
check_hostname = true

[faust]
# All keys from this section will be passed directly to the faust.App constructor
# See common/util.py:make_app for keys that cannot be set from this configuration
# See https://faust-streaming.github.io/faust/userguide/settings.html#guide-settings
producer_compression_type = "zstd"

[rdap-dn]
# -- Available in all components -- #
# Faust app ID
app_id = "domrad-rdap-dn"
# Faust debug mode
debug = false
# The number of worker threads that process input entries (available in all components)
concurrency = 4
# The main log level (TRACE/DEBUG/INFO/WARNING/ERROR/CRITICAL)
log_level = "DEBUG"
# The log level for sending logs to stderr
log_level_stderr = "TRACE"
# The log level for sending logs to Kafka
# Set to an empty string to disable Kafka logging
log_level_kafka = "WARNING"
# If true, Faust logs of the configured level will also be sent to Kafka
include_faust_logs_in_kafka = false
# --------------------------------- #

# RDAP request timeout (seconds)
http_timeout = 5

# See https://asynciolimiter.readthedocs.io/en/latest/ for details on the limiter.
# When 'immediate' is true, the input entries will immediately fail with the LOCAL_RATE_LIMIT error if the
# local rate limiter is empty. Otherwise, they will be queued until the limiter is free, up to 'max_wait' seconds.
# 'rate' is generally the number of allowed requests per second.
# This is the base setting, it can be overriden per-endpoint.
limiter = { immediate = false, max_wait = 10, type = "limiter", rate = 5, max_burst = 5 }
# limiter = { immediate = false, max_wait = -1, type = "strict", rate = 2 }
# limiter = { immediate = false, max_wait = -1, type = "leaky_bucket_meter", rate = 2, capacity = 5 }

[rdap-dn.limiter_overrides]
# Example of overriding the limiter settings for a specific RDAP or WHOIS endpoint.
# Note that the values of "immediate", "max_wait" and "type" will be re-used from the base setting if not specified
# but the limiter-specific keys must always be present!
# The key may be the full URL of the endpoint or the target TLD.
# If a TLD is used, the same configuration will be used both for its RDAP and WHOIS, although a separate limiter
# instance will be created for both.
"https://rdap.verisign.com/com/v1/" = { type = "leaky_bucket_meter", rate = 2, capacity = 10 }
"cz" = { max_wait = 10, type = "limiter", rate = 10, max_burst = 10 }
"whois.iana.org" = { max_wait = 5, type = "limiter", rate = 10, max_burst = 5 }

[rdap-ip]
app_id = "domrad-rdap-ip"
# RDAP request timeout (seconds)
http_timeout = 5
# Same as in rdap-dn.
limiter = { immediate = false, max_wait = 10, type = "limiter", rate = 5, max_burst = 5 }

[rdap-ip.limiter_overrides]
# Here, the keys must only contain the full URL of the RDAP endpoint.
# See https://data.iana.org/rdap/ipv4.json and https://data.iana.org/rdap/ipv6.json.
"https://rdap.lacnic.net/rdap/" = [5, 60]

[rtt]
app_id = "domrad-rtt"
ping_count = 5
privileged_mode = false

[dns]
app_id = "domrad-dns"
# The DNS servers to use for fallback resolution if the primary NS does not respond.
dns_servers = ["195.113.144.194", "195.113.144.233"]
# A single DNS query timeout.
timeout = 5
# Whether to query the fallback DNS servers in a round-robin fashion.
rotate_nameservers = false
# DNS record types to query for (supported types: A, AAAA, CNAME, MX, NS, TXT).
types_to_scan = ["A", "AAAA", "CNAME", "MX", "NS", "TXT"]
# DNS record types to further process IPs from (supported types: A, AAAA, CNAME, MX, NS).
types_to_process_IPs_from = ["A", "AAAA", "CNAME"]
# Maximum number of retries for a single DNS query in case of a timeout.
max_record_retries = 2
# Log level for the DNS scanner.
scanner_log_level = "INFO"
# EXPERIMENTAL: Use a single UDP socket instance for all DNS queries. If changed to false,
# each UDP query will open a new socket and close it after the response is received.
use_one_socket = true

[zone]
app_id = "domrad-zone"
# The DNS servers to use for queries needed for zone resolution.
dns_servers = ["195.113.144.194", "195.113.144.233"]
# A single DNS query timeout.
timeout = 5
# Whether to query the DNS servers in a round-robin fashion.
rotate_nameservers = true

[extractor]
app_id = "domrad-extractor"
data_dir = "/path/to/data"
# Number of records to collect in a buffer and process at once.
# This should be fairly high, according to the input rate.
# The downside of batching is that in case of an unhandled exception,
# the whole batch is lost.
batch_size = 100
# Maximum time (in seconds) to wait for a full batch before processing.
# In other words, the extractor will always process its current buffer
# `batch_timeout` seconds after the last processed one, even if it's not full.
batch_timeout = 5
# When set to > 1, the extraction process will be executing in a real thread from a thread pool
# with the specified number of threads. In normal world, this would help in parallelizing the extraction, but here,
# Python's GIL prevents this from happening, so this setting could only benefit from pandas running
# its computations in native code. However, the current transformations are dumb and mostly performed using .apply()
# which is executed under the GIL, so this setting should be probably be left at 1.
computation_threads = 1
# This property controls the applied transformations. The order is irelevant, it is set by the code.
# You probably don't want to change this. Leave the property out to use all available transformations.
enabled_transformations = ["dns", "ip", "geo", "tls", "lexical", "rdap_dn", "rdap_ip", "drop"]
# When set to true, the feature extractor will output the feature vectors one-by-one and serialized into JSON.
standalone_mode = false

[classifier-unit]
app_id = "domrad-classifier-unit"
# If not set, the directory embedded with the "domainradar-clf" project will be used
model_path = "/path/to/models"
# This controls the "batching of batches" coming from the feature extractor.
# It is recommended to only batch the data on the feature extractor level.
batch_size = 1
batch_timeout = 0